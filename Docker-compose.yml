services:
  redis:
    image: redis/redis-stack-server:latest
    ports:
      - "6379:6379"
    # volumes:
    #     - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "-p", "6379", "ping"]
      interval: 2s
      timeout: 1m30s
      retries: 5
      start_period: 5s

  langchain-chat-app:
    image: langchain-chat-app:latest
    build: ./
    command: chainlit run main_chainlid.py
    volumes:
      - ./models/:/modles/
    ports:
      - 8000:8080
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_COLLECTION=reports
      - MODEL_PATH=/modles/mistral-7B-v0.1/mistral-7b-instruct-v0.1.Q4_0.gguf
      - OPENAI_API_BASE=https://llm-api:8000
      - OPENAI_API_KEY="no need just for not exaption"

  llm-api: # Replace with your desired service name
    image: ghcr.io/chenhunghan/ialacol:latest
    ports:
      - "8000:8000"
    environment:
      - DEFAULT_MODEL_HG_REPO_ID=mistralai/Mistral-7B-Instruct-v0.1 
      # - DEFAULT_MODEL_FILE=llama-2-7b-chat.ggmlv3.q4_0.bin
    stdin_open: true # Equivalent to -it in docker run (interactive terminal)
    tty: true     





services:
  redis:
    image: redis/redis-stack-server:latest
    ports:
      - "6379:6379"
    # volumes:
    #     - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "-h", "localhost", "-p", "6379", "ping"]
      interval: 2s
      timeout: 1m30s
      retries: 5
      start_period: 5s

  langchain-chat-app:
    image: langchain-chat-app:latest
    build: ./
    command: chainlit run main_chainlid.py
    volumes:
      - ./models/:/modles/
    depends_on:
      llm-api:
        condition: service_healthy
    links: 
        - llm-api
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_COLLECTION=reports
      - MODEL_PATH=/modles/mistral-7B-v0.1/mistral-7b-instruct-v0.1.Q4_0.gguf
      - OPENAI_API_BASE=https://llm-api:3000
      - OPENAI_API_KEY="no need just for not exaption"

  llm-api: # Replace with your desired service name
    image: ghcr.io/chenhunghan/ialacol:latest
    ports:
      - "3000:8000"
    environment:
      - DEFAULT_MODEL_HG_REPO_ID=TheBloke/Mistral-7B-Instruct-v0.1-GGUF
      - DEFAULT_MODEL_FILE=mistral-7b-instruct-v0.1.Q5_K_M.gguf
    
    stdin_open: true # Equivalent to -it in docker run (interactive terminal)
    tty: true 
    healthcheck:
      test: ["CMD",
            "curl",
            "-X", "POST",
            "-H", "Content-Type: application/json",
            "-d", "{ \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a story.\"}], \"model\": \"mistral-7b-instruct-v0.1.Q5_K_M.gguf\", \"stream\": false, \"temperature\": \"2\", \"top_p\": \"1.0\", \"top_k\": \"0\" }",
            "http://localhost:8000/v1/chat/completions"
        ]
      interval: 5s
      timeout: 1m30s
      retries: 5
      start_period: 5s    




